write.csv(resultarray[, , 2], resraw_p)
JDraw_p <- file.path(path, "Day_of_year_raw.csv")
write.csv(resultarray[, , 3], JDraw_p)
IGRraw_p <- file.path(path, "Instantaneous_growth_rate_raw.csv")
write.csv(resultarray[, , 4], IGRraw_p)
SSTraw_p <- file.path(path, "SST_raw.csv")
write.csv(resultarray[, , 5], SSTraw_p)
d18OSDraw_p <- file.path(path, "Modeled_d18O_SD_raw.csv")
write.csv(resultarray[, , 6], d18OSDraw_p)
JDSDraw_p <- file.path(path, "Day_of_Year_SD_raw.csv")
write.csv(resultarray[, , 7], JDSDraw_p)
IGRSDraw_p <- file.path(path, "Instantaneous_growth_rate_SD_raw.csv")
write.csv(resultarray[, , 8], IGRSDraw_p)
SSTSDraw_p <- file.path(path, "SST_SD_raw.csv")
write.csv(resultarray[, , 9], SSTSDraw_p)
parraw_p <- file.path(path, "modeled_parameters_raw.csv")
write.csv(parmat, parraw_p)
}
# Write avay summary statistics of modeling
AMR_p <- file.path(path, "Age_model_results.csv")
write.csv(JDstats, AMR_p)
d18OR_p <- file.path(path, "d18O_model_results.csv")
write.csv(d18Ostats, d18OR_p)
GRR_p <- file.path(path, "Growth_rate_results.csv")
write.csv(GRstats, GRR_p)
SSTR_p <- file.path(path, "SST_results.csv")
write.csv(Tstats, SSTR_p)
parR_p <- file.path(path, "Model_parameter_results.csv")
write.csv(parstats, parR_p)
print("DONE!")}
export_path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/ShellChron_SG113"
# STEP 3: Align model resultis s to cumulative timescale
print("Calculating cumulative day of the year results...")
suppressWarnings(resultarray[, , 3] <- cumulative_day(resultarray, TRUE, TRUE, export_path)) # Calculate cumulative day of the year for all model runs and replace matrix in result array
# ----------ERROR------------------------------------
# STEP 4: Order and export results and statistics
export_results(export_path, dat, resultarray, parmat, MC, dynwindow, plot, plot_export, export_raw) # Export results of model
export_raw = TRUE
# STEP 3: Align model resultis s to cumulative timescale
print("Calculating cumulative day of the year results...")
suppressWarnings(resultarray[, , 3] <- cumulative_day(resultarray, TRUE, TRUE, export_path)) # Calculate cumulative day of the year for all model runs and replace matrix in result array
rm(list=ls())
load("E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/20220813_SG113_SG.RData")
export_results <- function(path = getwd(), # Path where result files are exported
dat, # raw data
resultarray, # Array containing all model results
parmat, # matrix of parameters per window
MC = 1000, # Include number of simulations just for error verification (if MC > 0, errors are included in the export)
dynwindow, # Include the size of the windows for pooling standard deviations AND FOR ADDING WEIGHINGS TO STATISTICS BASED ON PLACE IN WINDOW
plot = FALSE, # Create a result plot?
plot_export = TRUE, # Export a result plot?
export_raw = FALSE # Export all the raw results of the model (of individual windows)?
){
# Set export path
oldwd <- getwd()
on.exit(setwd(oldwd))
setwd(export_path)
Day <- sd.day <- N <- se.day <- d18O_mod <- sd.d18O_mod <- se.d18O_mod <-
GR <- sd.GR <- se.GR <- SST <- sd.SST <- se.SST <- parameter <-
par_value <- stdev <- se.pars <- SD <- d18Oc <- mean.day <- CL95.day <-
mean.d18O_mod <- CL95.d18O_mod <- mean.GR <- CL95.GR <- NULL # Predefine variables to circumvent global variable binding error
# Define weights to give more priority to datapoints in the center of the modeling window than those on the edge
weights <- matrix(NA, ncol = length(dynwindow$x), nrow = length(dynwindow$x) + dynwindow$y[length(dynwindow$x)] - 1) # Create template matrix
for(i in 1:length(dynwindow$x)){ # Loop through matrix and add weights for each position in the resultarray that contains a value
weights[dynwindow$x[[i]]:(dynwindow$x[[i]] + dynwindow$y[[i]] - 1), i] <- dynwindow$y[[i]] / 2 - abs(dynwindow$x[[i]]:(dynwindow$x[[i]] + dynwindow$y[[i]] - 1) - (dynwindow$x[[i]] + (dynwindow$y[[i]] - 1) / 2))
}
weights <- cbind(resultarray[, -grep("window", colnames(resultarray[, , 3])), 3], weights)
weightstidy <- tidyr::gather(as.data.frame(weights), "window", "weight", (ncol(dat) + 1):ncol(weights), factor_key = TRUE) # Convert weights to Tidy data for plotting
JDtidy <- tidyr::gather(as.data.frame(resultarray[, , 3]), "window", "Day", (ncol(dat) + 1):length(resultarray[1, , 1]), factor_key = TRUE) # Convert modeled time results to Tidy data for plotting
JDtidy$weights <- weightstidy$weight # Add weights to JDtidy
JDstats <- JDtidy %>% # Summarize modeled time statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
mean.day = mean(Day, na.rm = TRUE),  # Calculate means per sample
sd.day = sd_wt(Day, weights, na.rm = TRUE),  # Calculate stdevs per sample
N = dplyr::n_distinct(Day, na.rm = TRUE), # Calculate the number of modeled values, excluding NA's
se.day = sd.day / sqrt(N), # Calculate the standard error
CL95.day = qt(0.95, N) * se.day # Calculate the 95% confidence level
)
JDstats$sd.day[which(JDstats$N == 1)] <- NaN
d18Otidy <- tidyr::gather(as.data.frame(resultarray[, , 1]), "window", "d18O_mod", (ncol(dat) + 1):length(resultarray[1, , 1]), factor_key = TRUE) # Convert modeled d18O results to Tidy data for plotting
d18Otidy$weights <- weightstidy$weight # Add weights to d18Otidy
d18Ostats <- d18Otidy %>% # Summarize modeled d18O statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
mean.d18O_mod = mean(d18O_mod, na.rm = TRUE),  # Calculate means per sample, excluding NA's
sd.d18O_mod = sd_wt(d18O_mod, weights, na.rm = TRUE),  # Calculate stdevs per sample, excluding NA's
N = dplyr::n_distinct(d18O_mod, na.rm = TRUE), # Calculate the number of modeled values, excluding NA's
se.d18O_mod = sd.d18O_mod / sqrt(N), # Calculate the standard error
CL95.d18O_mod = qt(0.95, N) * se.d18O_mod # Calculate the 95% confidence level
)
d18Ostats$sd.d18O_mod[which(d18Ostats$N == 1)] <- NaN
GRtidy <- tidyr::gather(as.data.frame(resultarray[, , 4]), "window", "GR", (ncol(dat) + 1):length(resultarray[1, , 1]), factor_key = TRUE) # Convert modeled growth rate results to Tidy data for plotting
GRtidy$weights <- weightstidy$weight # Add weights to GRtidy
GRstats <- GRtidy %>% # Summarize modeled growth rate statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
mean.GR = mean(GR[GR > 0.1], na.rm = TRUE),  # Calculate means per sample, excluding NA's and instances where growth rate is near-zero
sd.GR = sd_wt(GR[GR > 0.1], weights, na.rm = TRUE),  # Calculate stdevs per sample, excluding NA's and instances where growth rate is near-zero
N = dplyr::n_distinct(GR[GR > 0.1], na.rm = TRUE), # Calculate the number of modeled values, excluding NA's and instances where growth rate is near-zero
se.GR = sd.GR / sqrt(N), # Calculate the standard error
CL95.GR = qt(0.95, N) * se.GR # Calculate the 95% confidence level
)
GRstats$sd.GR[which(GRstats$N == 1)] <- NaN
Ttidy <- tidyr::gather(as.data.frame(resultarray[, , 5]), "window", "SST", (ncol(dat) + 1):length(resultarray[1, , 1]), factor_key = TRUE) # Convert modeled temperature results to Tidy data for plotting
Ttidy$weights <- weightstidy$weight # Add weights to Ttidy
Tstats <- Ttidy %>% # Summarize modeled growth rate statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
mean.SST = weighted.mean(SST, na.rm = TRUE),  # Calculate means per sample, excluding NA's
sd.SST = sd_wt(SST, weights, na.rm = TRUE),  # Calculate stdevs per sample, excluding NA's
N = dplyr::n_distinct(SST, na.rm = TRUE), # Calculate the number of modeled values, excluding NA's
se.SST = sd.SST / sqrt(N), # Calculate the standard error
CL95.SST = qt(0.95, N) * se.SST # Calculate the 95% confidence level
)
Tstats$sd.SST[which(Tstats$N == 1)] <- NaN
parmat2 <- data.frame(rownames(parmat), parmat)
colnames(parmat2)[1] <- "parameter"
partidy <- tidyr::gather(parmat2, "window", "par_value", 2:ncol(parmat2), factor_key = TRUE)
parstats <- partidy %>% # Summarize model parameters
ggpubr::group_by(parameter) %>%
dplyr::summarize(
means = mean(par_value), # Calculate means per parameter
stdev = sd(par_value), # Calculate standard deviation per parameter
N = dplyr::n(), # Count number of modeled values per parameter (= equal to number of windows)
se.pars = stdev / sqrt(N), # Calculate standard error
CL95 = qt(0.95, N) * se.pars
)
if(MC > 0){
print("Recalculating export statistics by including propagated uncertainties")
# Include errors propagated from those on D and d18Oc data into the statistics
# Propagate errors on modeled d18O
d18Otidy_err <- d18Otidy
d18Otidy_err$SD <- tidyr::gather(as.data.frame(resultarray[, , 6]), "window", "d18O", (ncol(dat) + 1):length(resultarray[1, , 1]), factor_key = TRUE)$d18O # Convert modeled time errors to Tidy data for plotting
d18Otidy_err$N <- dynwindow$y[as.numeric(d18Otidy$window)] # Add window size for calculating pooled SD
d18Otidy_err <- d18Otidy_err[-which(is.na(d18Otidy_err$d18O_mod)), ] # Remove empty cells in matrix
d18Otidy_err$SD[which(d18Otidy_err$SD == 0)] <- min(d18Otidy_err$SD[which(d18Otidy_err$SD > 0)]) # Replace zeroes with smallest SD to prevent division by zero
d18Ostats2 <- d18Otidy_err %>% # Summarize modeled d18O statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
weighted.mean.d18O = weighted.mean(d18O_mod, 1 / SD ^ 2 * weights, na.rm = TRUE),  # Calculate weighted means per sample
pooled.sd.d18O = sqrt(sum(SD ^ 2 * (N - 1) * weights, na.rm = TRUE) / ((sum(N, na.rm = TRUE) - dplyr::n()) * mean(weights))) # Calculate pooled standard deviation resulting from error propagations and the weighted mean of the variances taking weights derived from position in the window into account
)
# Aggregate propagated errors into statistics matrices
d18Ostats$mean.d18O_mod <- d18Ostats2$weighted.mean.d18O # Replace means by weighed means, taking into account the propagated error on individual estimates
d18Ostats$sd.d18O_mod <- sqrt(d18Ostats$sd.d18O_mod ^ 2 + d18Ostats2$pooled.sd.d18O ^ 2) # Combine errors from the model and the errors on input
d18Ostats$se.d18O_mod <- d18Ostats$sd.d18O_mod / sqrt(d18Ostats$N) # Propagate new errors onto standard error
d18Ostats$CL95.d18O_mod <- qt(0.95, d18Ostats$N) * d18Ostats$se.d18O_mod # Propagate new errors onto confidence interval
# Propagate errors on Time of Day calculations
JDtidy_err <- JDtidy
JDtidy_err$SD <- tidyr::gather(as.data.frame(resultarray[, , 7]), "window", "Day", (ncol(dat) + 1):length(resultarray[1, , 1]), factor_key = TRUE)$Day # Convert modeled time errors to Tidy data for plotting
JDtidy_err$N <- dynwindow$y[as.numeric(JDtidy$window)] # Add window size for calculating pooled SD
JDtidy_err <- JDtidy_err[-which(is.na(JDtidy_err$Day)), ] # Remove empty cells in matrix
JDtidy_err$SD[which(JDtidy_err$SD == 0)] <- min(JDtidy_err$SD[which(JDtidy_err$SD > 0)]) # Replace zeroes with smallest SD to prevent division by zero
JDstats2 <- JDtidy_err %>% # Summarize modeled JD statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
weighted.mean.day = weighted.mean(Day, 1 / SD ^ 2 * weights, na.rm = TRUE),  # Calculate weighted means per sample
pooled.sd.day = sqrt(sum(SD ^ 2 * (N - 1) * weights, na.rm = TRUE) / ((sum(N, na.rm = TRUE) - dplyr::n()) * mean(weights))) # Calculate pooled standard deviation resulting from error propagations and the weighted mean of the variances taking weights derived from position in the window into account
)
# Aggregate propagated errors into statistics matrices
JDstats$mean.day <- JDstats2$weighted.mean.day # Replace means by weighed means, taking into account the propagated error on individual estimates
JDstats$sd.day <- sqrt(JDstats$sd.day ^ 2 + JDstats2$pooled.sd.day ^ 2) # Combine errors from the model and the errors on input
JDstats$se.day <- JDstats$sd.day / sqrt(JDstats$N) # Propagate new errors onto standard error
JDstats$CL95.day <- qt(0.95, JDstats$N) * JDstats$se.day # Propagate new errors onto confidence interval
# Propagate errors on modeled growth rate
GRtidy_err <- GRtidy
GRtidy_err$SD <- tidyr::gather(as.data.frame(resultarray[, , 8]), "window", "GR", (ncol(dat) + 1):length(resultarray[1, , 1]), factor_key = TRUE)$GR # Convert modeled time errors to Tidy data for plotting
GRtidy_err$N <- dynwindow$y[as.numeric(GRtidy$window)] # Add window size for calculating pooled SD
GRtidy_err <- GRtidy_err[-which(is.na(GRtidy_err$GR)), ] # Remove empty cells in matrix
GRtidy_err$SD[which(GRtidy_err$SD == 0)] <- min(GRtidy_err$SD[which(GRtidy_err$SD > 0)]) # Replace zeroes with smallest SD to prevent division by zero
GRstats2 <- GRtidy_err %>% # Summarize modeled GR statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
weighted.mean.GR = weighted.mean(GR, 1 / SD ^ 2 * weights, na.rm = TRUE),  # Calculate weighted means per sample
pooled.sd.GR = sqrt(sum(SD ^ 2 * (N - 1) * weights, na.rm = TRUE) / ((sum(N, na.rm = TRUE) - dplyr::n()) * mean(weights))) # Calculate pooled standard deviation resulting from error propagations and the weighted mean of the variances taking weights derived from position in the window into account
)
# Aggregate propagated errors into statistics matrices
GRstats$mean.GR <- GRstats2$weighted.mean.GR # Replace means by weighed means, taking into account the propagated error on individual estimates
GRstats$sd.GR <- sqrt(GRstats$sd.GR ^ 2 + GRstats2$pooled.sd.GR ^ 2) # Combine errors from the model and the errors on input
GRstats$se.GR <- GRstats$sd.GR / sqrt(GRstats$N) # Propagate new errors onto standard error
GRstats$CL95.GR <- qt(0.95, GRstats$N) * GRstats$se.GR # Propagate new errors onto confidence interval
# Propagate errors on modeled temperature
Ttidy_err <- Ttidy
Ttidy_err$SD <- tidyr::gather(as.data.frame(resultarray[, , 9]), "window", "T", (ncol(dat) + 1):length(resultarray[1, , 1]), factor_key = TRUE)$T # Convert modeled time errors to Tidy data for plotting
Ttidy_err$N <- dynwindow$y[as.numeric(Ttidy$window)] # Add window size for calculating pooled SD
Ttidy_err <- Ttidy_err[-which(is.na(Ttidy_err$SST)), ] # Remove empty cells in matrix
Ttidy_err$SD[which(Ttidy_err$SD == 0)] <- min(Ttidy_err$SD[which(Ttidy_err$SD > 0)]) # Replace zeroes with smallest SD to prevent division by zero
Tstats2 <- Ttidy_err %>% # Summarize modeled T statistics
ggpubr::group_by(D) %>%
dplyr::summarize(
weighted.mean.SST = weighted.mean(SST, 1 / SD ^ 2 * weights, na.rm = TRUE),  # Calculate weighted means per sample
pooled.sd.SST = sqrt(sum(SD ^ 2 * (N - 1) * weights, na.rm = TRUE) / ((sum(N, na.rm = TRUE) - dplyr::n()) * mean(weights))) # Calculate pooled standard deviation resulting from error propagations and the weighted mean of the variances taking weights derived from position in the window into account
)
# AgTegate propagated errors into statistics matrices
Tstats$mean.SST <- Tstats2$weighted.mean.SST # Replace means by weighed means, taking into account the propagated error on individual estimates
Tstats$sd.SST <- sqrt(Tstats$sd.SST ^ 2 + Tstats2$pooled.sd.SST ^ 2) # Combine errors from the model and the errors on input
Tstats$se.SST <- Tstats$sd.SST / sqrt(Tstats$N) # Propagate new errors onto standard error
Tstats$CL95.SST <- qt(0.95, Tstats$N) * Tstats$se.SST # Propagate new errors onto confidence interval
print("Preparing plots")
}
if(plot == TRUE | plot_export == TRUE){ # Check if plots are needed
# Create depth-time plot
Dtplot <- ggplot2::ggplot(JDtidy, ggplot2::aes(D, Day)) +
ggplot2::geom_point(ggplot2::aes(colour = d18Oc)) +
ggplot2::scale_colour_gradient2(midpoint = mean(JDtidy$d18Oc)) +
ggplot2::geom_line(data = JDstats, ggplot2::aes(D, mean.day), size = 1) +
ggplot2::geom_line(data = JDstats, ggplot2::aes(D, mean.day + CL95.day), size = 1, alpha = 0.5) +
ggplot2::geom_line(data = JDstats, ggplot2::aes(D, mean.day - CL95.day), size = 1, alpha = 0.5) +
ggplot2::ggtitle("Plot of Height vs. Time") +
ggplot2::xlab("Record length") +
ggplot2::scale_y_continuous("Age (days)", seq(0, 365 * ceiling(max(JDstats$mean.day + JDstats$CL95.day, na.rm = TRUE) / 365), 365))
# Create d18O plot
d18Oplot <- ggplot2::ggplot(d18Otidy, ggplot2::aes(D, d18Oc)) +
ggplot2::geom_point() +
ggplot2::geom_line(data = d18Ostats, ggplot2::aes(D, mean.d18O_mod), size = 1) +
ggplot2::geom_line(data = d18Ostats, ggplot2::aes(D, mean.d18O_mod + CL95.d18O_mod, alpha = 0.5, col = "darkblue"), size = 1) +
ggplot2::geom_line(data = d18Ostats, ggplot2::aes(D, mean.d18O_mod - CL95.d18O_mod, alpha = 0.5, col = "darkred"), size = 1) +
ggplot2::ggtitle("Plot of measured and modeled d18O vs. Record Length") +
ggplot2::xlab("Record length") +
ggplot2::ylab("d18O_carbonate") +
ggplot2::theme(legend.position = "none") # Remove legend
# Create growth rate plot
GRplot <- ggplot2::ggplot(GRtidy, ggplot2::aes(D, GR)) +
ggplot2::geom_point(ggplot2::aes(colour = d18Oc)) +
ggplot2::scale_colour_gradient2(midpoint = mean(JDtidy$d18Oc)) +
ggplot2::geom_line(data = GRstats, ggplot2::aes(D, mean.GR), size = 1) +
ggplot2::geom_line(data = GRstats, ggplot2::aes(D, mean.GR + CL95.GR, alpha = 0.5), size = 1) +
ggplot2::geom_line(data = GRstats, ggplot2::aes(D, mean.GR - CL95.GR, alpha = 0.5), size = 1) +
ggplot2::ggtitle("Plot of modeled growth rate vs Record Length") +
ggplot2::xlab("Record length") +
ggplot2::ylab("Growth rate") +
ggplot2::theme(legend.position = "none") # Remove legend
Combined_plots <- ggpubr::ggarrange(Dtplot, d18Oplot, GRplot, labels = c("A", "B", "C"), ncol = 3, nrow = 1) # Combine plots
if(plot == TRUE){
dev.new()
print(Combined_plots)
}
if(plot_export == TRUE){
pdf("Model result plots.pdf", width = 30, height = 10)
print(Combined_plots)
dev.off()
}
}
print("Start exporting files to directory")
if(export_raw == TRUE){
# Write away all raw results of modeling
d18Oraw_p <- file.path(path, "modeled_d18O_raw.csv")
write.csv(resultarray[, , 1], d18Oraw_p)
resraw_p <- file.path(path, "residuals_raw.csv")
write.csv(resultarray[, , 2], resraw_p)
JDraw_p <- file.path(path, "Day_of_year_raw.csv")
write.csv(resultarray[, , 3], JDraw_p)
IGRraw_p <- file.path(path, "Instantaneous_growth_rate_raw.csv")
write.csv(resultarray[, , 4], IGRraw_p)
SSTraw_p <- file.path(path, "SST_raw.csv")
write.csv(resultarray[, , 5], SSTraw_p)
d18OSDraw_p <- file.path(path, "Modeled_d18O_SD_raw.csv")
write.csv(resultarray[, , 6], d18OSDraw_p)
JDSDraw_p <- file.path(path, "Day_of_Year_SD_raw.csv")
write.csv(resultarray[, , 7], JDSDraw_p)
IGRSDraw_p <- file.path(path, "Instantaneous_growth_rate_SD_raw.csv")
write.csv(resultarray[, , 8], IGRSDraw_p)
SSTSDraw_p <- file.path(path, "SST_SD_raw.csv")
write.csv(resultarray[, , 9], SSTSDraw_p)
parraw_p <- file.path(path, "modeled_parameters_raw.csv")
write.csv(parmat, parraw_p)
}
# Write avay summary statistics of modeling
AMR_p <- file.path(path, "Age_model_results.csv")
write.csv(JDstats, AMR_p)
d18OR_p <- file.path(path, "d18O_model_results.csv")
write.csv(d18Ostats, d18OR_p)
GRR_p <- file.path(path, "Growth_rate_results.csv")
write.csv(GRstats, GRR_p)
SSTR_p <- file.path(path, "SST_results.csv")
write.csv(Tstats, SSTR_p)
parR_p <- file.path(path, "Model_parameter_results.csv")
write.csv(parstats, parR_p)
print("DONE!")}
cumulative_day <- function(resultarray, # Align Day of year results from modeling in different windows to a common time axis
plotyearmarkers = TRUE, # Plot peak fitting?
export_peakid = TRUE, # Export data on how the boundaries between years were found?
path = tempdir()
){
dat <- as.data.frame(resultarray[, -grep("window", colnames(resultarray[, , 3])), 3]) # isolate original data
# Recognition of the boundaries between years.
# Method one: Apply sinusoidal function to the julian day simulations
JDdat <- resultarray[, (ncol(dat) + 1):length(resultarray[1, , 1]), 3] # Isolate julian day simulations
JDdat <- matrix(sin(2 * pi * (JDdat + 365 / 4) / 365), ncol = ncol(JDdat)) # Convert julian day to sinusoidal value (end and start of year = 1)
JDends <- data.frame(Depth = dat$D,
Yearmarker = dat$YEARMARKER,
YEsin = scales::rescale(rowSums(JDdat, na.rm = TRUE), c(0, 1)) # Create depth series of positions which most likely represent a year end, rescale to a scale from 0 to 1
)
# Method two: Give weight to the first and final days in reconstructions
weightvector <- c(seq(10, 1, -1), rep(0, 346), seq(1, 10, 1)) # Create vector of weights to be given to days near the start and end of the year
JDdat <- round(resultarray[, (ncol(dat) + 1):length(resultarray[1, , 1]), 3]) # Isolate julian day simulations
JDdat <- matrix(weightvector[JDdat + 1], ncol = ncol(JDdat)) # Apply weighting to starts and ends of the year
JDends$YEweight <- scales::rescale(rowSums(JDdat, na.rm = TRUE), c(0, 1)) # Add normalized results to depth series
# Method three: Use instances within the window simulations where the end of year is recorded
JDdat <- resultarray[, (ncol(dat) + 1):length(resultarray[1, , 1]), 3] # Isolate julian day simulations
JDdat <- rbind(rep(NA, length(JDdat[1, ])), diff(JDdat) < 0) # Matrix of positions in individual windows where end of year (day 365) is recorded
YEcount <- rowSums(JDdat, na.rm = TRUE) / max(rowSums(JDdat, na.rm = TRUE), na.rm = TRUE) # Aggregate the counted ends of years in simulations
smoothfactor <- min(diff(which(JDends$Yearmarker == 1))) # Define smoothing factor for year end counts based on yearmarkers (take thickness of year with least growth to be conservative)
YEcount <- c(rep(0, floor(smoothfactor / 2)), zoo::rollmean(YEcount, smoothfactor, align = "center"), rep(0, smoothfactor - floor(smoothfactor / 2) - 1)) # Smooth record of year end counts and pad with zeroes
JDends$YEcount <- scales::rescale(YEcount, c(0, 1)) # Add normalized results to depth series
# Method four: Use maxima in d18Oc in original data
yearpos <- c(1, which(JDends$Yearmarker == 1), length(JDends$Depth)) # Extract positions of yearmarkers and include start and end of record
yearpos <- unique(yearpos) # Remove duplicates in yearpos due to yearmarkers on beginning and/or end of record
YE18O <- vector() # Create vector for the position of the maximum d18O value
for(m in 1:(length(yearpos) - 1)){ # Loop through positions of yearmarkers
if(m %in% 2:(length(yearpos) - 2)){ # central part of the record
maxpos <- which(dat$d18Oc[yearpos[m] : (yearpos[m + 1] - 1)] == max(dat$d18Oc[yearpos[m] : (yearpos[m + 1] - 1)])) + yearpos[m] - 1 # Find the position of the maximum value in the d18O data in that year
if(length(maxpos) > 1){
maxpos = round(stats::median(maxpos)) # Prevent multiple values in maxpos (gives errors further in the calculations)
}
days <- seq(1, yearpos[m + 1] - yearpos[m], 1) * 365 / (yearpos[m + 1] - yearpos[m]) # Define sequence of "days" values with length = number of datapoints in the year
sinusoid <- sin(2 * pi * (days - rep((maxpos - yearpos[m]) / (yearpos[m + 1] - yearpos[m]) * 365 - 365 / 4, length(days))) / 365) # Create sinusoid with peak at peak in d18Oc
sinusoid[which(days > ((maxpos - yearpos[m]) / (yearpos[m + 1] - yearpos[m]) + 0.5) * 365 | days < ((maxpos - yearpos[m]) / (yearpos[m + 1] - yearpos[m]) - 0.5) * 365)] <- -1 # Assign lowest value (-1) to all datapoints more than 1/2 period away from the maximum to prevent false peaks
YE18O <- append(YE18O, sinusoid) # add sinusoid values to running vector
}else if(m == 1){ # beginning of the record
maxpos <- which(dat$d18Oc[yearpos[m + 1] : (yearpos[m + 2] - 1)] == max(dat$d18Oc[yearpos[m + 1] : (yearpos[m + 2] - 1)])) + yearpos[m + 1] - 1 # Find the position of the maximum value in the d18O data of the next year (the first year that is complete)
if(length(maxpos) > 1){
maxpos = round(stats::median(maxpos)) # Prevent multiple values in maxpos (gives errors further in the calculations)
}
days <- seq(yearpos[m] - yearpos[m + 1] + 1, yearpos[m + 2] - yearpos[m + 1], 1) * 365 / (yearpos[m + 2] - yearpos[m + 1]) # Define sequence of "days" values
sinusoid <- sin(2 * pi * (days - rep((maxpos - yearpos[m + 1]) / (yearpos[m + 2] - yearpos[m + 1]) * 365 - 365 / 4, length(days))) / 365) # Create sinusoid with peak at peak in d18Oc
sinusoid[which(days > ((maxpos - yearpos[m + 1]) / (yearpos[m + 2] - yearpos[m + 1]) + 0.5) * 365 | days < ((maxpos - yearpos[m + 1]) / (yearpos[m + 2] - yearpos[m + 1]) - 0.5) * 365)] <- -1 # Assign lowest value (-1) to all datapoints more than 1/2 period away from the maximum to prevent false peaks
YE18O <- append(YE18O, sinusoid[1:(yearpos[m + 1] - yearpos[m])]) # Add sinusoid values for first bit of record to the vector
}else if(m == (length(yearpos) - 1)){ # end of the record
maxpos <- which(dat$d18Oc[yearpos[m - 1] : (yearpos[m] - 1)] == max(dat$d18Oc[yearpos[m - 1] : (yearpos[m] - 1)])) + yearpos[m - 1] - 1 # Find the position of the maximum value in the d18O data of the previous year (the last year that is complete)
if(length(maxpos) > 1){
maxpos = round(stats::median(maxpos)) # Prevent multiple values in maxpos (gives errors further in the calculations)
}
days <- seq(1, yearpos[m + 1] - yearpos[m - 1], 1) * 365 / (yearpos[m] - yearpos[m - 1]) # Define sequence of "days" values
sinusoid <- sin(2 * pi * (days - rep((maxpos - yearpos[m - 1]) / (yearpos[m] - yearpos[m - 1]) * 365 - 365 / 4, length(days))) / 365) # Create sinusoid with peak at peak in d18Oc
sinusoid[which(days > ((maxpos - yearpos[m - 1]) / (yearpos[m] - yearpos[m - 1]) + 0.5) * 365 | days < ((maxpos - yearpos[m - 1]) / (yearpos[m] - yearpos[m - 1]) - 0.5) * 365)] <- -1 # Assign lowest value (-1) to all datapoints more than 1/2 period away from the maximum to prevent false peaks
YE18O <- append(YE18O, sinusoid[(yearpos[m] - yearpos[m - 1]):(yearpos[m + 1] - yearpos[m - 1])]) # Add sinusoid values for last bit of record to the vector
}
}
JDends$YE18O <- scales::rescale(YE18O, c(0, 1)) # Add normalized results to depth series
JDends$YEcombined <- scales::rescale(rowSums(JDends[, -c(1, 2)]), c(0, 1)) # Combine all four methods of peak recognition into one vector
# Use peak ID algorhythm to find peaks in the combined vector of year end indicators to use as basis for cumulative day counting in the model results
wmin <- round(min(diff(which(JDends$Yearmarker == 1))) / 4) # Define starting window for peak recognition as one forth the minimum width of a year
wmax <- round(min(diff(which(JDends$Yearmarker == 1))) / 2) # Define maximum window for peak recognition as half the minimum width of a year
YM <- length(which(JDends$Yearmarker == 1)) # Extract number of years in record
X <- list(w = vector(), p = vector()) # List for storing results
w <- wmin # Start at minimum window size
repeat{
peaks <- peakid(JDends$Depth, JDends$YEcombined, w = w, span = 0.05) # Identify peaks based on current threshold
if(length(peaks$x) == YM){ # Check if the number of years is correct and break loop if it is
break
}else if(w < wmax){ # Check if maximum window is reached
X$w <- append(X$w, w) # Store window size
X$p <- append(X$p, length(peaks$x)) # Store peak number
w <- w + 1 # Increment window size
}else{
X$w <- append(X$w, w) # Store window size
X$p <- append(X$p, length(peaks$x)) # Store peak number
w <- max(X$w[which(abs(X$p - YM) == min(abs(X$p - YM)))]) # If no windows give the exact number of years, take the window that comes closer (prioritize larger windows in case of a tie)
peaks <- peakid(JDends$Depth, JDends$YEcombined, w = w, span = 0.05) # Recalculate peak positions with the final chosen window size before breaking the loop
break
}
}
if(length(peaks$x) != YM){
# If peak algorithm fails (it often does for small datasets), try again with a larger span
YM <- length(which(JDends$Yearmarker == 1)) # Extract number of years in record
X <- list(w = vector(), p = vector()) # List for storing results
w <- wmin # Start at minimum window size
repeat{
peaks <- peakid(JDends$Depth, JDends$YEcombined, w = w, span = 0.1) # Identify peaks based on current threshold
if(length(peaks$x) == YM){ # Check if the number of years is correct and break loop if it is
break
}else if(w < wmax){ # Check if maximum window is reached
X$w <- append(X$w, w) # Store window size
X$p <- append(X$p, length(peaks$x)) # Store peak number
w <- w + 1 # Increment window size
}else{
X$w <- append(X$w, w) # Store window size
X$p <- append(X$p, length(peaks$x)) # Store peak number
w <- max(X$w[which(abs(X$p - YM) == min(abs(X$p - YM)))]) # If no windows give the exact number of years, take the window that comes closer (prioritize larger windows in case of a tie)
peaks <- peakid(JDends$Depth, JDends$YEcombined, w = w, span = 0.1) # Recalculate peak positions with the final chosen window size before breaking the loop
break
}
}
}
JDends$peakid <- rep(0, length(JDends$Depth)) # Add column for peakid results
JDends$peakid[peaks$i] <- 1 # Mark the location of peaks found in the time series
if(plotyearmarkers == TRUE){
dev.new()
graphics::plot(JDends$Depth,
JDends$YEcombined,
type = "l",
main = "Peak fitting results",
xlab = "Depth",
ylab = "Probability of end of year") # Plot aggregate of yearmarkers
points(peaks$x, rep(1, length(peaks$x)), col = "red") # Plot location of yearmarkers
}
if(export_peakid == TRUE){
write.csv(JDends, file.path(path, "peakid.csv"))
}
# Apply calculation of years in the model on the simulation results
JDdat <- resultarray[, (ncol(dat) + 1):length(resultarray[1, , 1]), 3] # Isolate julian day simulations
for(col in 1:length(JDdat[1, ])){ # Loop through all simulation windows
window <- JDdat[, col] # Isolate simulation window
peakx <- head(which(peaks$x %in% dat$D[which(!is.na(window))]), 1) # Find position of the first year end in the window column
if(length(peakx) == 0){
if(peaks$x[1] < dat$D[head(which(!is.na(window)), 1)]){ # If no global year transitions fall within the window, check if there are global year transitions before the window and find the last year transition before the first value in the window
peakx <- max(which(peaks$x < dat$D[head(which(!is.na(window)), 1)])) # Find number of years before start of window
JDpeak <- window[head(which(!is.na(window)), 1)] # Find the day of year of the first simulated value
if(JDpeak < 182.5){
window <- window + peakx * 365 # If first simulated value co-occurs with first half of the year, add all years before the window
}else{
window <- window + (peakx - 1) * 365 # If year first simulated value co-occurs with last half of the year, simulated values are assumed to belong to the previous year
}
} # If all year transitions happen after the window, no year number needs to be added to the window values
}else{
JDpeak <- JDdat[tail(which(dat$D == peaks$x[peakx] & !is.na(window)), 1), col] # Find JD simulation belonging to the first year transition
if(JDpeak < 182.5){
window <- window + peakx * 365 # If year transition co-occurs with first half of the simulated year, all simulated values are assumed to belong to the next year
}else{
window <- window + (peakx - 1) * 365 # If year transition co-occurs with last half of the simulated year, all simulated values are assumed to belong to the previous year
}
}
if(length(which(diff(window) < 0)) > 0){ # Check if year transitions occur within the simulation
for(i in which(diff(window) < 0)){
if(length(peakx) > 0){ # Check if global year transitions occur within the window
if(i < which(dat$D == peaks$x[peakx])){
window[1:i] <- window[1:i] - 365 # If the transition happens before the reference point (peakx), subtract a year's worth of days from the values before to prevent adding a year twice.
}else{
window[(i + 1):length(window)] <- window[(i + 1):length(window)] + 365 # If the transition happens on or after the reference point (peakx), add one year's worth of days to simulations after each transition
}
}else{
window[(i + 1):length(window)] <- window[(i + 1):length(window)] + 365 # If no global transitions occur within the window, add one year's worth of days to simulations after each local transition
}
}
}
JDdat[, col] <- window # Update the julian day data with the new cumulative day simulations
}
# Screen for extreme outliers in JD due to very bad simulations based on first and last modeled values
firstval <- apply(JDdat, 2, min, na.rm = TRUE) # find first values of all modeling windows
lastval <- apply(JDdat, 2, max, na.rm = TRUE) # find last values of all modeling windows
badwindow <- rep(FALSE, ncol(JDdat)) # Vector indicating if window simulation is bad
for(i in 2:(ncol(JDdat) - 1)){
if(((abs(firstval[i] - firstval[i - 1]) > 365) & (abs(firstval[i + 1] - firstval[i])) > 365) | ((abs(lastval[i] - lastval[i - 1]) > 365) & (abs(lastval[i + 1] - lastval[i])))){ # Find windows with large (> 1 year) jumps in either first or last value
badwindow[i] <- TRUE # Label these windows as bad
}
}
JDdat[, badwindow] <- rep(NA, nrow(JDdat)) # Remove values from bad windows to prevent messing up the age model
result <- resultarray[, , 3]
result[, (ncol(dat) + 1):length(result[1, ])] <- JDdat # Add the updated cumulative julian day results to the resultarray and export
return(result)
}
export_path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/ShellChron_SG113"
export_raw <- TRUE
print("Calculating cumulative day of the year results...")
suppressWarnings(resultarray[, , 3] <- cumulative_day(resultarray, TRUE, TRUE, export_path)) # Calculate cumulative day of the year for all model runs and replace matrix in result array
export_results(export_path, dat, resultarray, parmat, MC, dynwindow, plot, plot_export, export_raw) # Export results of model
devtools::document()
devtools::install()
require(tidyverse)
require(ShellChron)
All_sample_data <- read.csv("E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/PWP_sample_data.csv")
# Extract sample numbers and take average for pooled samples
All_sample_data$Sample_nr2 <- NA
for(i in 1:nrow(All_sample_data)){
if(!is.na(as.numeric(All_sample_data$Sample_nr[i]))){
# If sample is a single-number sample, just return the sample number
All_sample_data$Sample_nr2[i] <- as.numeric(All_sample_data$Sample_nr[i])
} else if(!is_empty(grep(";", All_sample_data$Sample_nr[i]))){
# Find instances of multiple sample numbers separated by semicolons
# Return the average of the pooled sample numbers
All_sample_data$Sample_nr2[i] <- mean(as.numeric(strsplit(All_sample_data$Sample_nr[i], ";")[[1]]))
} else if(!is_empty(grep("-", All_sample_data$Sample_nr[i]))){
# Find instances of a range of sample numbers separated by a dash
# Return the average of the pooled sample numbers
All_sample_data$Sample_nr2[i] <- mean(as.numeric(strsplit(All_sample_data$Sample_nr[i], "-")[[1]]))
}
}
# Apply shellchron on species to create chronology based on d18Oc
SG113 <- subset(All_sample_data, Specimen == "SG113" & OUTLIER == FALSE)
SG113sg <- SG113 %>%
select(
D = Sample_nr2,
d18Oc = Final_d18O,
d18Oc_err = d18O_sd_ext
) %>%
mutate(
D = D * 1000,
D_err = 500,
YEARMARKER = 0
)
SG113sg$YEARMARKER[which(SG113sg$D  %in% c(1000, 6000, 13000, 23000))] <- 1 # Add year markers
wrap_function(
path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/ShellChron_SG113",
input_from_file = FALSE, # Should input be read from a file?
object_name = SG113sg, # Name of object with input (only if input_from_file = FALSE)
transfer_function = "KimONeil97", # Set transfer function of the record, default is Kim and O'Neil 1997.
t_int = 1, # Set time interval in days
T_per = 365, # Set annual time period in days (default = 365)
d18Ow = 0, # Set d18Ow value or vector (default = constant year-round at 0 VSMOW). Alternative options are either one value (assumed constant year-round) or a vector with length T_per / t_int and interval t_int specifying d18Ow evolution through one year.
t_maxtemp = 182.5, # Define the day of the year at which temperature is heighest. Default = Assume that the day of maximum temperature is helfway through the year
SCEUApar = c(1, 25, 10000, 5, 0.01, 0.01), # Set parameters for SCEUA optimization (iniflg, ngs, maxn, kstop, pcento, peps)
sinfit = TRUE, # Apply sinusoidal fitting to guess initial parameters for SCEUA optimization? (TRUE/FALSE)
MC = 1000, # Number of MC simulations to include measurement error into error analysis. Default = 1000 (if MC = 0, error on D and d18O measurements not considered)
plot = TRUE, # Should intermediate plots be given to track progress? WARNING: plotting makes the script much slower, especially for long datasets.
plot_export = TRUE, # Should a plot of the results be saved as PDF?
export_raw = FALSE, # Should the results of all individual model runs be exported as CSV files?
export_path = "E:/Dropbox/Research/postdoc/UNBIAS/PWP reconstructions/Combined_data/ShellChron_SG113"
)
devtools::document()
devtools::install()
